---
title: "PUBPOL 6090 Lecture 5 Prep"
author: "Thelonious Goerz"
date: "2023-09-04"
output: pdf_document
---

1.  **OLS Part II: Binary Dependent Data**

Setup: Probit and logit are common link functions for binary dependent variables. However, we can also use linear regression.

**Ways to use OLS**

1.  We can predict y
2.  We can see how our prediction changes if we change x

**We can use LPM**

with $pr(Y=1) = X\beta + \epsilon$

-   With regression we get CEF

-   With binary, E(Y) = pr(Y=1)

    -   Which means that the interpretation that we get is "the probability that Y is one as a function of our RHS variables"

*Problems with LPM*

-   We can get predicted values outside (0,1)

    -   If we are not interested in *predicting* Y then it is not an issue -- if we just want the slope

-   Te data distribution is heteroskedastic

    -   Generally $var(\epsilon) = Pr(Y=1) \times (1-pr(Y=1))$

    -   Necessary to use robust se

-   The slope is constant even if the data have an S shape

    -   What it gives is essentially an average of the derivatives

**Frisch Waugh Lovell theorem**

-   Called partitioned and partial regression

-   Regression anatomy

*Basic setup*

In a linear regression $Y = \beta_1 + \beta_2 + \epsilon$

-   We are interested in $\beta_1$

Approach

-   Regress $Y$ on $X2$

-   Regress X1 on X2

-   Call residuals resid_X1

-   Regress resid_Y on Resid_x_1

-   The coefficients will be the same as $\beta_1$ in equation 1

Applications

-   Dummy variables and deviations from means

    -   If we have a bunch of fixed effects, it is easier to run the regressions on a bunch of i level means, then take the residuals and estimate the regression on the rest of the Xs

        -   $(Y_i - \bar{Y}_i)$

-   Graphical representation

    -   bivariate plots that are "partial correlations"

**To Do**

-   Look through MHE and find regression anatomy formula (p. 35)

-   Find FWL in C&T

------------------------------------------------------------------------

2.  **Potential outcomes framework**

-   Rubin causal model, developed by Don Rubin (Neymen too)

    -   Think about the $Y_i$ outcome variable, with potential outcomes, we are going to explicitly represent both the observed process and the unobserved counterfactual (which are the potential outcomes)

-   There is also the treatment $T_i \in (0,1)$

    -   Potential outcomes are $\{ Y_{0i}, Y_{1i}$

        -   We can think of these outcomes as fixed states in a world where I am treated and I am not treated

-   We observe

$$
Y_i = Y_{0i} (T_i = 0) \ + \ Y_i = Y_{1i}(T_i =1) \\
\Delta_i = Y_{1i} - Y_{i0}
$$

-   $\Delta_i$ is unobserved

In practice we target

-   The ATE $E[\Delta_i]$

-   Or the ATT $E[\Delta_i|T_i = 1]$ also called treatment on treated

-   We can also do $E[\Delta_i|T_i=0]$ which is equivalent to suddenly treating all utreated

-   $E[\Delta_i| induce]$

    -   Which informally, is the effect of being induced to treat for individuals on the margin

        -   Imagine a scenario where we lower the barriers to schooling slightly, so we want to know what happens when a small nuge happens to induce treatment. Suppose we lower school costs and some people sign up, we can look at the treatment effect for the group that signed up.

            -   This is often called a *Local Average Treatment Effect*

            -   In theory there can be different local average treatment effects for a continuous nonlinear relationship

-   Rather than assuming a constant treatment effect. current thinking is about which estimators yield which target quantities.

*Estimating the causal effect of treatment*

Maybe we can estimate the ATE by estimating$\bar{Y_{i=1}} - \bar{Y_{i=0}}$ , plim $E(Y_i|T_i=1) - E(Y_{i=0} |T_i = 0)$

By the add and subtract trick

$$
= E(Y_{1i} |T_i =1) - {E(Y_i |T_i =1)} \ + \bf{E(Y_{0i}|T_i = 1) - E(Y_{0i} | T_i =0)}
$$

The first, non bold, term the ATT

$$
E(\Delta_i|T_=1) 
$$

The second term is a selection bias term. We are comparing the same potential outcome $Y_{0i}$ in the setting where the individual is treated and not.

-   If, the people that were not treated are different than those treated, this estimator will be biased.

In the form of regression

$$
1: \ Y_i = \beta_0 + \beta_1 + \tau_i + \epsilon \\
2 :\ Y_{0i} = \beta_0 + \epsilon \\
3 :\ Y_{1i} = \beta_0 + \beta_1 + \epsilon \\
4: \ E(Y_{0i}|T=1) - E(Y_{0i} |T=0) = E(\epsilon|T=1) - E(\epsilon|T=0) \\
or Cor(\epsilon,T) \ineq 0
$$

Term four will be present if the errors between the treatment and control groups are different, if they are the same then they equal 0 and cancel out like $\beta_0$

-   PE does not impose strong distributional or functional form assumptions

Assumption of randomization is that Being randomized to a group

$$
R =1 = E(Y_{1i} | T_=1) \\
R = 0 = E(Y_{0i} |T_i=0) \\
$$

Randomization ensures that one's potential outcomes are independent of their treatment assignment. This can go wrong however. If we rely on randomization to $=$ treatment, then we have an ITT analysis.

IF we take the ratio of the difference in Y given randomization and the treatment given randomization, we get a sort of late which is the treatment effect for people who take up the treatment whose treatment changes due to R.

$$
\frac{E(|R=1)-E(Y|R=0)}{E(T|R=1) - E(T|R=0)}
$$

One way to estimate this equation is to run an instrumental variables estimation using Z as an instrument for T.
